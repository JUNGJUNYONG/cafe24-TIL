## OS캐시와 분산

+ OS 캐시와 원리
+ 캐시를 이용한 I/O 부하를 줄이는 방법 및 분산
+ 국소성을 살리는 분산



### OS 캐시의 구조

+ OS 캐시란?

  - 디스크와 메모리 간의 **속도차**를 줄이기 위해 OS가 메모리를 이용해서 디스크의 액세스를 줄이는 원리
  - Linux 는 page cache, file cache, buffer cache 세 종류의 캐시 구조를 갖춤.
    * [참고] 앞으로는 page cache로 캐시를 설명

+ 가상 메모리의 구조

  + 정의 : 논리적인 선형 어드레스를 물리적인 물리 어드레스로 변환하는 기능하는 구조
  + 목적 : 물리적인 하드웨어를 OS에서 **추상화**하기 위해
  + 가상 메모리 구조 흐름
    1. 프로세스에서 메모리를 요청
    2. OS는 메모리에서 비어있는 곳을 찾음
    3. OS는 비어있는 어드레스를 프로세스에 반환
       - 개별 프로세스에선 메모리를 '0x0000' 번지부터 시작한다고 알려주는 것이 다루기 쉬움
       - **OS는** 메모리를 직접 프로세스로 넘기는 것이 아니라 일단 **커널 내에서 메모리를 추상화**한다.
       - OS는 메모리를 4KB단위로 **블록(= 페이지)** 을 만들어 프로세스에게 반환
    4. 프로세스는 OS에게 받은 빈 어드레스를 이용하여 일을 수행
   
  + 이점 : 프로세스에서 메모리를 쉽게 다룰 수 있다.
  + 페이지 : OS가 물리 메모리를 확보/관리 하는 단위

+ Linux 페이지 캐시 원리

  + OS는 확보한 페이지를 메모리상에 계속 확보해두는 기능을 가짐.

  + 페이지 캐시 원리 

    1. 프로세스가 요청하면 OS는 디스크로부터 4KB 크기의 블록(페이지)을 읽는다 ==  **페이지가 작성된다. **
    2. 맨 처음 읽는 데이터는 메모리상에 적재 → 프로세스가 액세스할 수 있는 것은 (가상) 메모리 뿐
    3. OS는 적재 된 메모리 주소를(가상 어드레스) 프로세스에 알려줌
    4. 프로세스는 가상 어드레스로 데이터를 액세스 함

    - **OS는 메모리에 적재 된 페이지를 삭제하지 않고 남겨 둔다** ▶ **페이지 캐시**

  + 페이지 캐시의 이점

    + 메모리에 데이터를 삭제하지 않고 보관되어 다시 똑같은 데이터 요청시 디스크로 부터 다시 액세스 하지 않으므로 **액세스 시간이 단축된다.**

+ VFS(Virtual File System)

  + OS와 디바이스 드라이버사이에는 여러 파일 시스템들이(ext3, ext2, ext4, xfs) 존재
  + 파일 시스템 위에 VFS라는 추상화 레이어가 있어서 **여러 파일시스템들의 인터페이스를 하나로 통일**
  + VFS가 페이지 캐시의 구조를 가지고 있다.

+ Linux는 페이지 단위로 캐시

  + 페이지 캐시는 4KB단위로 캐싱하므로 특정 파일의 일부분 또는 읽어낸 부분만을 캐싱할 수 있다. 
  + LRU : 메모리 용량이 꽉 채워져 있고, 새로운 캐시를 저장할 때, 가장 오래된 것을 파기하여 새로운 캐시를 적재하는 방법

+ Linux의 sar 명령어로 캐싱 확인하기

  ![1](uploads/152788b4b584f7c96fa8ea4e48a2a5c0/1.PNG)

  <center> 부팅 된 후 메모리 상태 </center>

    ![2](uploads/fffc88cbde4f76cf8a4c5dab013d6bcc/2.PNG)

  <center> iptables 파일 열기 </center>

    ![3](uploads/4192f77a8ff8221de464beda18a9fd79/3.PNG))

  <center> 파일 연 후 메모리 상태 </center>


  + %memused(메모리 사용량)과 kbcached(캐시된 데이터 용량)이 증가한 것을 확인

  + **OS는 디스크 액세스 요청이 올 때마다 메모리에 디스크 내용을 캐싱한다**

  + **OS는 부팅 후, 자동적으로 디스크의 데이터를 캐싱하는 것이 아니라 프로세스가 데이터를 요청 할 때 마다 디스크의 데이터를 메모리에 캐싱한다.**

+ 메모리를 늘려서 I/O 부하 줄이기
  + 메모리를 늘리면 캐시에 사용할 수 있는 용량이 늘어남
  + 캐시 용량이 늘어나면 많은 데이터를 캐싱할 수 있음
  + **캐싱한 데이터가 많으면 디스크를 읽는 횟수가 줄어듬 ▶ I/O 부하가 줄어든다. **

## I/O 부하를 줄이는 방법

+ 캐시를 전제로 한 I/O 줄이는 방법
  + 데이터 규모 < 물리 메모리 이면 전부 캐싱 할 수 있다.
    + 데이터 압축하여 저장하면 디스크 내용을 전부 그대로 메모리에 캐싱해 둘 수 있다.
  + 경제적 비용과 밸런스 고려
+ 복수 서버로 확장시키기
  - 캐시로 해결될 수 없을 정도로 규모가 커졌을 때는 복수 서버로 확장하는 방법 고려
  - DB서버를 늘릴 경우는 부하의 경우도 있지만 **캐시 용량을 늘리기 위한** 경우도 있다.
  - CPU 부하분산은 서버를 늘리면 해결
  - I/O 분산에는 국소성을 고려
+ 단순히 대수를 늘려선 확장성을 확보할 수 없음
  + **대수만을 늘린 것은 단순 데이터를 복사하므로 캐싱할 수 없는 비율은 변함이 없다.**
  + 다시 I/O 부하가 발생한다.

## 국소성을 살리는 분산

+ 국소성을 고려한 분산

  + 정의 : 서버에 단순히 데이터를 복제해서 늘리는 것이 아닌, **데이터에 대한 액세스 패턴**을 고려하여 분산하는 것이다.
  + 이점 : 캐싱할 수 없는 부분이 사라져 더 많은 데이터를 캐싱할 수 있다.

+ 파티셔닝 - 국소성을 고려한 분산 1

  + 파티셔닝이란 ?

    : DB서버를 여러 대의 서버로 분할하는 방법

  + 테이블 단위 분할

    : 같이 요청이 들어오는 테이블끼리 같은 DB서버에 저장하여 분할하는  방법

  + 테이블 데이터 분할

    : 테이블 하나를 여러 테이블로 잘게 잘라 각기 다른 DB서버에 저장하는 방법

    + ex) id를 저장하는 테이블을 id 첫 문자를 기준으로 테이블을 잘게 잘라 DB서버에 저장
    + 분할의 크기를 조절할 때 데이터를 병합해야하는 번거로움

+ 요청 패턴을 '섬'으로 분할 - 국소성을 고려한 분산 2

  + 용도별로 시스템을 섬으로 나눈 방법

    + HTTP 요청을 User-Agent나 URL 등으로 구분하여 용도별로 섬으로 분할 하여 나누는 방법

      > 나의 개인적인 견해로는, DB서버는 Master와  슬레이브 구조로 되어있고, Master에선  모든 데이터를 관리하고  슬레이브를  요청별로  나누어  섬으로 구성하고 있지 않을까 생각한다.. 

+ 페이지 캐시를 고려한 운용의 기본 규칙
  + OS 가동 직후에 서버를 투입하지 않는다.
    + 캐시가 쌓여있지 않으므로 디스크 액세스만 발생하게 되어 서버가 내려갈 수 있음
    + OS를 시작하고 자주 사용되는  DB파일을 한 번에 cat 해준다.
  + 성능평가는 캐시가 최적화 되어 있을 때 한다.
